---
title: Model Context Protocol
description: Learn about how you can use Model Context Protocol with the Rig library.
---

# Using Model Context Protocol (MCP) with Rig

The **Model Context Protocol (MCP)** by Anthropic provides a standardized way for AI agents to dynamically discover, register, and invoke external tools through a common interface. This enables tools to be treated as first-class citizens in LLM-based workflows and allows for seamless integration between agents and external capabilities.

## What is MCP?

MCP is a protocol that enables language models and agents to:

- Discover available tools from a remote server.
- Initialize and declare their capabilities.
- Invoke registered tools and receive responses.

It defines both **client** and **server** roles. The **server** hosts tools and responds to invocations, while the **client** discovers and uses those tools.

## Getting started

Creating an MCP client requires the `rmcp` crate to be installed. You can run the following one-line command below in your terminal to install it quickly:

```bash
cargo add mcp-core
```

You will also additionally need the `rmcp` feature enabled on the `rig-core` crate. If you haven't done so already, you can run the following one-line command in your terminal to add it to your project:

```bash
cargo add rig-core -F rmcp
```

### Initialising an MCP client

To connect to the server and fetch tool metadata, you need to create a client using the `rmcp` crate. The complete unabridged example code can be found [from the GitHub repo.](https://github.com/0xPlaygrounds/rig/blob/main/rig-core/examples/rmcp.rs)

You will also need the `rmcp` crate enabled with the following features:

```toml
rmcp = { version = "0.8", features = [
    "client",
    "macros",
    "transport-streamable-http-client",
    "transport-streamable-http-client-reqwest",
    "transport-streamable-http-server-session",
    "transport-streamable-http-server",
    "transport-worker",
] }
```

Next, you'll need to add the following code to your program which spawns a transport format for streamable HTTP, then serves the client information to the server and returns a conection.

```rust
let transport =
    rmcp::transport::StreamableHttpClientTransport::from_uri("http://localhost:8080");

let client_info = ClientInfo {
    protocol_version: Default::default(),
    capabilities: ClientCapabilities::default(),
    client_info: Implementation {
        name: "rig-core".to_string(),
        version: "0.23.0".to_string(),
    },
};

let client = client_info.serve(transport).await.inspect_err(|e| {
    tracing::error!("client error: {:?}", e);
})?;
```

### Listing MCP tools

Once initialized, the client can list tools available on the MCP server:

```rust
let tools: Vec<Tool> = client.list_tools(Default::default()).await?.tools;

println!("Tools: {:?}", tools);
```

### Using MCP tools

Now that you've retrieved the tool list from the MCP server, you can now pass it into a Rig agent:

```rust
let completion_model = providers::openai::Client::from_env();

let agent = completion_model
    .agent("gpt-4o")
    .rmcp_tools(tools, client.peer().to_owned())
    .build();

let response = agent.prompt("Add 10 + 10").await?;
tracing::info!("Agent response: {:?}", response);
```

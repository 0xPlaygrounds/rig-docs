{"/about":{"title":"About","data":{"":"This is the about page! This page is shown on the navbar."}},"/docs/quickstart":{"title":"Quickstart","data":{"introduction#Introduction":"In the rapidly evolving landscape of artificial intelligence (AI), Large Language Models (LLMs) have emerged as powerful tools for building sophisticated AI applications. However, harnessing the full potential of LLMs often requires navigating complex APIs, managing different providers, and implementing intricate workflows. This is where Rig comes in â€“ a comprehensive Rust library designed to transform how developers build LLM-powered applications.","the-challenge-of-building-llm-applications#The Challenge of Building LLM Applications":"Before diving into Rig's capabilities, let's consider the challenges developers face when building LLM applications:\nAPI Complexity: Each LLM provider has its own API, requiring developers to learn and manage multiple interfaces.\nWorkflow Management: Implementing advanced AI workflows, such as Retrieval-Augmented Generation (RAG), involves multiple steps and can be error-prone.\nPerformance and Scalability: Ensuring optimal performance and scalability in LLM applications can be challenging, especially as projects grow in complexity.\nType Safety and Error Handling: Maintaining type safety and robust error handling across different LLM interactions is crucial but often difficult.","enter-rig-a-game-changer-for-llm-application-development#Enter Rig: A Game-Changer for LLM Application Development":"Rig is more than just an API wrapper; it's a comprehensive framework that addresses these challenges head-on. By providing high-level abstractions and a unified interface, Rig simplifies the development process, allowing you to focus on building innovative AI solutions rather than wrestling with implementation details.Whether you're a seasoned Rust developer or new to the language, Rig offers a range of features designed to make your LLM application development smoother, faster, and more enjoyable.","getting-started-with-rig#Getting Started with Rig":"Let's dive into a simple example to demonstrate how easy it is to get started with Rig:\nuse rig::{completion::Prompt, providers::openai};\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<(), anyhow::Error> {\r\n    // Initialize the OpenAI client using environment variables\r\n    let openai_client = openai::Client::from_env();\r\n    \r\n    // Create a GPT-4 model instance\r\n    let gpt4 = openai_client.model(\"gpt-4\").build();\r\n    \r\n    // Send a prompt to GPT-4 and await the response\r\n    let response = gpt4.prompt(\"Explain quantum computing in one sentence.\").await?;\r\n    \r\n    // Print the response\r\n    println!(\"GPT-4: {}\", response);\r\n    \r\n    Ok(())\r\n}\nThis simple example demonstrates how Rig abstracts away the complexities of interacting with OpenAI's API, allowing you to focus on the core logic of your application.To include Rig in your project, add the following to your Cargo.toml:\n[dependencies]\r\nrig-core = \"0.0.6\"\r\ntokio = { version = \"1.34.0\", features = [\"full\"] }\nðŸ’¡ Tip: Don't forget to set the OPENAI_API_KEY environment variable before running your application.","key-features-and-developer-experience#Key Features and Developer Experience":"Rig combines Rust's powerful type system and performance with intuitive abstractions tailored for AI development. Let's explore some of its key features:","1-unified-and-intuitive-api#1. Unified and Intuitive API":"One of Rig's standout features is its consistent interface across different LLM providers:\n// Using OpenAI\r\nlet gpt4 = openai_client.model(\"gpt-4\").build();\r\nlet response = gpt4.prompt(\"Hello, GPT-4!\").await?;\r\n\r\n// Using Cohere\r\nlet command = cohere_client.model(\"command\").build();\r\nlet response = command.prompt(\"Hello, Cohere!\").await?;\nThis unified API design ensures that switching between providers or adding new ones to your project is seamless, reducing cognitive load and improving code maintainability.","2-advanced-abstractions-for-complex-workflows#2. Advanced Abstractions for Complex Workflows":"Rig shines when it comes to implementing complex AI workflows. For example, creating a Retrieval-Augmented Generation (RAG) system typically involves multiple steps:\nGenerating embeddings for documents\nStoring these embeddings in a vector database\nRetrieving relevant context based on user queries\nAugmenting the LLM prompt with this context\nWith Rig, this entire process can be condensed into a few lines of code:\nlet rag_agent = openai_client.context_rag_agent(\"gpt-4\")\r\n    .preamble(\"You are a helpful assistant.\")\r\n    .dynamic_context(2, vector_store.index(embedding_model))\r\n    .build();\r\n\r\nlet response = rag_agent.prompt(\"What is the capital of France?\").await?;\nThis high-level abstraction allows developers to implement advanced AI systems quickly and efficiently, without getting bogged down in the implementation details.","3-type-safe-development#3. Type-Safe Development":"Leveraging Rust's strong type system, Rig provides compile-time guarantees and better auto-completion, enhancing the developer experience:\n#[derive(serde::Deserialize, JsonSchema)]\r\nstruct Person {\r\n    name: String,\r\n    age: u8,\r\n}\r\n\r\nlet extractor = openai_client.extractor::<Person>(\"gpt-4\").build();\r\nlet person: Person = extractor.extract(\"John Doe is 30 years old\").await?;\nThis type-safe approach helps catch errors early in the development process and makes refactoring and maintenance easier.","4-extensibility-and-integration#4. Extensibility and Integration":"Rig's flexible architecture allows for easy customization and seamless integration with Rust's growing AI ecosystem:\nimpl VectorStore for MyCustomStore {\r\n    // Implementation details...\r\n}\r\n\r\nlet my_store = MyCustomStore::new();\r\nlet rag_agent = openai_client.context_rag_agent(\"gpt-4\")\r\n    .dynamic_context(2, my_store.index(embedding_model))\r\n    .build();\nThis extensibility ensures that Rig can grow with your project's needs and integrate with other tools in your AI development stack.","advanced-features-rag-systems-and-beyond#Advanced Features: RAG Systems and Beyond":"Let's explore a more comprehensive example of a RAG system with Rig, showcasing its ability to handle complex AI workflows:\nuse rig::{\r\n    completion::Prompt,\r\n    embeddings::EmbeddingsBuilder,\r\n    providers::openai::Client,\r\n    vector_store::{in_memory_store::InMemoryVectorStore, VectorStore},\r\n};\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<(), anyhow::Error> {\r\n    // Initialize OpenAI client and embedding model\r\n    let openai_client = Client::from_env();\r\n    let embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\r\n\r\n    // Create and populate vector store\r\n    let mut vector_store = InMemoryVectorStore::default();\r\n    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\r\n        .simple_document(\"doc1\", \"Rig is a Rust library for building LLM applications.\")\r\n        .simple_document(\"doc2\", \"Rig supports OpenAI and Cohere as LLM providers.\")\r\n        .build()\r\n        .await?;\r\n    vector_store.add_documents(embeddings).await?;\r\n\r\n    // Create and use RAG agent\r\n    let rag_agent = openai_client.context_rag_agent(\"gpt-4\")\r\n        .preamble(\"You are an assistant that answers questions about Rig.\")\r\n        .dynamic_context(1, vector_store.index(embedding_model))\r\n        .build();\r\n\r\n    let response = rag_agent.prompt(\"What is Rig?\").await?;\r\n    println!(\"RAG Agent: {}\", response);\r\n\r\n    Ok(())\r\n}\nThis example demonstrates how Rig abstracts the complexity of creating a RAG system, handling embedding generation, vector storage, and context retrieval efficiently. With just a few lines of code, you've implemented a sophisticated AI system that can provide context-aware responses.But Rig's capabilities extend beyond RAG systems. Its flexible architecture allows for the implementation of various AI workflows, including:\nMulti-agent systems for complex problem-solving\nAI-powered data analysis and extraction\nAutomated content generation and summarization\nAnd much more!"}},"/docs/Architecture":{"title":"Architecture","data":{}},"/":{"title":"Get Started with Rig","data":{"":"Rig is a Rust library for building portable, modular, and lightweight Fullstack AI Agents. You can find API documentation on docs.rs."}},"/guides":{"title":"Index","data":{"":"Explore our collection of blog posts and tutorials to learn how to build powerful LLM applications with Rig in Rust."}},"/guides/discord_bot":{"title":"Build an AI Discord Bot with Rust and Rig: Step-by-Step Guide","data":{"":"TL;DR: This comprehensive guide walks you through creating an AI-powered Discord bot using Rust and the Rig library. You'll learn to set up your environment, build a language model agent, and integrate it with Discord. By the end, you'll have an AI-powered chatbot that answers questions based on your own documents, provides coding assistance, and serves as an automated support tool.","introduction#Introduction":"Welcome to the next installment of the Build with Rig series. In this hands-on tutorial, we'll construct a fully functional AI Discord bot using Rust and the Rig library. Our bot will be capable of:\nAnswering user queries using a custom Markdown-based knowledge base\nOffering coding assistance and explanations\nServing as an automated customer service or community assistance tool\nThroughout this guide, we'll cover:\nSetting up your Rust development environment\nImplementing a language model agent with Rig\nIntegrating the bot with Discord\nDeploying and testing the bot\nWhile this guide assumes some familiarity with Rust, LLMs, and Discord, don't worry if this is your first Rust project. We'll focus on practical implementation, explaining key concepts and design decisions along the way.By the end of this tutorial, you'll have a working AI Discord bot and a solid grasp of building LLM-powered applications with Rig.Let's dive in.\nðŸ’¡ If you're new to Rig and want to start from the beginning or are looking for additional tutorials, check out our blog series.","prerequisites#Prerequisites":"Before we begin building, ensure you have the following:\nRust: If you haven't already, install Rust.\nDiscord Account and Bot Setup: You'll need a Discord account and a bot application. Follow the Discord bot setup guide.\nOpenAI API Key: To enable AI capabilities, integrate the OpenAI API by obtaining an API key from OpenAI. Sign up for an OpenAI API key.\nDon't worry if you're new to Rigâ€”we'll cover everything you need to know as we progress. For more detailed information about Rig, visit our GitHub repository or explore the Rig documentation.\nImportant: Never commit your API keys or .env files to version control. Ensure your .gitignore file includes these files to prevent accidental exposure.","project-setup#Project Setup":"With the prerequisites in place, let's set up our Rust project and install the necessary dependencies.","1-initialize-a-new-rust-project#1. Initialize a New Rust Project":"Open your terminal and run the following commands:\ncargo new discord_rig_bot\r\ncd discord_rig_bot\nThis creates a new Rust project called discord_rig_bot and navigates into the project directory.","2-add-dependencies#2. Add Dependencies":"Open the Cargo.toml file in your project directory and add the following dependencies under the [dependencies] section:\n[dependencies]\r\nrig-core = \"0.2.1\" # [Rig Crate](https://crates.io/crates/rig-core)\r\ntokio = { version = \"1.34.0\", features = [\"full\"] }\r\nserenity = { version = \"0.11\", default-features = false, features = [\"client\", \"gateway\", \"rustls_backend\", \"cache\", \"model\", \"http\"] }\r\ndotenv = \"0.15.0\"\r\nanyhow = \"1.0.75\"\r\ntracing = \"0.1\"\r\ntracing-subscriber = \"0.3\"\r\nreqwest = { version = \"0.11\", features = [\"json\"] }\r\nserde = { version = \"1.0\", features = [\"derive\"] }\r\nserde_json = \"1.0\"\r\nschemars = \"0.8\"\r\nasync-trait = \"0.1.83\"\nThese dependencies play crucial roles in our project:\nRig: The core library for building language model applications, simplifying the integration of AI capabilities into our Discord bot.\nSerenity: A third-party library for interacting with the Discord API.\nTokio: An asynchronous runtime for Rust, allowing our bot to handle multiple tasks concurrently.\nAdditional crates for error handling (anyhow), logging (tracing, tracing-subscriber), making HTTP requests (reqwest), and serialization (serde, serde_json, schemars).","understanding-the-bot-architecture#Understanding the Bot Architecture":"Our bot consists of two main components that work together to provide an intelligent and interactive user experience:\nRig Agent (rig_agent.rs): The core of our bot's magic touch. The Rig agent manages AI interactions, handles natural language processing, retrieves relevant information from a Markdown-based knowledge base using Rig's integrated Retrieval-Augmented Generation (RAG) capabilities, and generates contextually appropriate responses.\nDiscord Bot (main.rs): The interface between our AI and users. The Discord bot manages communication with Discord, listens for user commands and messages, and sends the generated responses back to the user.","message-processing-flow#Message Processing Flow":"To understand how our bot works, let's walk through the message processing flow:\nUser Input: A user sends a message or command in a Discord channel where the bot is present.\nDiscord Bot: The bot, always listening for new messages, receives the user's input and passes it to the Rig agent for processing.\nRig Agent: The agent processes the user's input, retrieves relevant information from the knowledge base, and generates an appropriate response using its language understanding and generation capabilities.\nResponse: The Discord bot receives the generated response from the Rig agent and sends it back to the user in the Discord channel.\nHere's a simplified diagram of the message processing flow:\nFor an in-depth look at building a RAG system with Rig, refer to our comprehensive article on building a simple RAG system with Rig.","building-the-rig-agent-rig_agentrs#Building the Rig Agent (rig_agent.rs)":"The Rig agent is the brain of our bot, responsible for understanding user queries, retrieving relevant information, and generating intelligent responses. Let's build it step by step.","1-create-the-rig_agentrs-file#1. Create the rig_agent.rs File":"In your src directory, create a new file named rig_agent.rs. This file will contain the implementation of our Rig agent.","2-import-necessary-modules#2. Import Necessary Modules":"At the top of rig_agent.rs, import the required modules:\n// rig_agent.rs\r\n\r\nuse anyhow::{Context, Result};\r\nuse rig::providers::openai;\r\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\r\nuse rig::embeddings::EmbeddingsBuilder;\r\nuse rig::rag::RagAgent;\r\nuse std::path::Path;\r\nuse std::fs;\r\nuse std::sync::Arc;\nThese modules provide the necessary functionality for our Rig agent, including error handling (anyhow), OpenAI language models and embeddings (rig::providers::openai), vector storage (rig::vector_store::in_memory_store), embedding generation (rig::embeddings::EmbeddingsBuilder), and the agent (rig::rag::RagAgent).","3-define-the-rigagent-struct#3. Define the RigAgent Struct":"Create the RigAgent struct that will manage the retrieval and response generation:\npub struct RigAgent {\r\n    rag_agent: Arc<RagAgent<openai::CompletionModel, rig::vector_store::InMemoryVectorIndex<openai::EmbeddingModel>, rig::vector_store::NoIndex>>,\r\n}\nThe RigAgent struct contains an Arc (Atomic Reference Counting) pointer to a RagAgent. The Arc type allows multiple parts of your program to share ownership of the same data in a thread-safe way.\nNote: Arc stands for Atomic Reference Counting. It is used for sharing data between threads safely.","4-implement-the-new-method#4. Implement the new Method":"The new method is responsible for initializing the Rig agent, setting up the OpenAI client, loading and embedding the knowledge base documents, and creating the RAG agent.Before we proceed, think about why we use Arc here. Since our bot will handle multiple asynchronous events, sharing the RigAgent across different parts of the program without transferring ownership is crucial. Arc provides a thread-safe way to do this.\nimpl RigAgent {\r\n    pub async fn new() -> Result<Self> {\r\n        // Initialize OpenAI client\r\n        let openai_client = openai::Client::from_env();\r\n        let embedding_model = openai_client.embedding_model(\"text-embedding-3-small\");\r\n\r\n        // Create vector store\r\n        let mut vector_store = InMemoryVectorStore::default();\r\n\r\n        // Get the current directory and construct paths to markdown files\r\n        let current_dir = std::env::current_dir()?;\r\n        let documents_dir = current_dir.join(\"documents\");\r\n\r\n        let md1_path = documents_dir.join(\"Rig_guide.md\");\r\n        let md2_path = documents_dir.join(\"Rig_faq.md\");\r\n        let md3_path = documents_dir.join(\"Rig_examples.md\");\r\n\r\n        // Load markdown documents\r\n        let md1_content = Self::load_md_content(&md1_path)?;\r\n        let md2_content = Self::load_md_content(&md2_path)?;\r\n        let md3_content = Self::load_md_content(&md3_path)?;\r\n\r\n        // Create embeddings and add to vector store\r\n        let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\r\n            .simple_document(\"Rig_guide\", &md1_content)\r\n            .simple_document(\"Rig_faq\", &md2_content)\r\n            .simple_document(\"Rig_examples\", &md3_content)\r\n            .build()\r\n            .await?;\r\n\r\n        vector_store.add_documents(embeddings).await?;\r\n\r\n        // Create index\r\n        let context_index = vector_store.index(embedding_model);\r\n\r\n        // Create RAG agent\r\n        let rag_agent = Arc::new(openai_client.context_rag_agent(\"gpt-4\")\r\n            .preamble(\"You are an advanced AI assistant powered by [Rig](https://rig.rs/), a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base.\r\n\r\n                    Key responsibilities and behaviors:\r\n                    1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information.\r\n                    2. Clarity and Conciseness: Provide clear and concise answers. Ensure responses are short and to the point. Use bullet points or numbered lists for complex information when appropriate.\r\n                    3. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information.\r\n                    4. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example:\r\n                        ```rust\r\n                        let example_code = \\\"This is how you format Rust code for Discord\\\";\r\n                        println!(\\\"{}\\\", example_code);\n\")\r\n.dynamic_context(2, context_index)\r\n.build());Ok(Self )\r\n}// ... we'll add more code here as we build things out\r\n}\n\r\nLet's break down the key steps:\r\n\r\n- **Initialize OpenAI Client**: The OpenAI client is set up using the API key stored in the environment variables. This client is essential for accessing OpenAI's language models and embedding services.\r\n\r\n- **Embedding Model**: The `text-embedding-3-small` model is selected for generating document embeddings. This model creates compact vector representations of text, enabling efficient semantic search and retrieval.\r\n\r\n- **Vector Store**: An in-memory vector store is created to hold and manage the document embeddings. Vector stores are optimized for fast similarity searches, allowing the agent to quickly find relevant information based on the user's query.\r\n\r\n- **Load Documents**: Markdown files containing the knowledge base are loaded from the `documents` directory. In this example, we have three files: `Rig_guide.md`, `Rig_faq.md`, and `Rig_examples.md`. These files contain information about the Rig library, frequently asked questions, and usage examples.\r\n\r\n- **Create Embeddings**: The loaded documents are converted into vector embeddings using the `EmbeddingsBuilder` provided by Rig. These embeddings capture the semantic meaning of the documents, enabling the agent to understand and retrieve relevant information based on the user's query.\r\n\r\n- **Agent Creation**: A `RagAgent` is created by combining the language model (GPT-4) with the vector store containing the document embeddings. The agent is capable of retrieving relevant information from the knowledge base and generating contextually appropriate responses.\r\n\r\n- **Preamble**: A carefully crafted preamble sets up the assistant's behavior and guidelines. The preamble defines the agent's primary function, responsibilities, and expected behaviors, ensuring that it provides accurate, concise, and technically proficient responses.\r\n\r\n> **Tip**: For more advanced configurations and techniques, such as implementing custom vector stores or configuring custom agents and tools, refer to the [official Rig examples](https://github.com/0xPlaygrounds/rig/tree/main/rig-core/examples).\r\n\r\n### 5. Implement the `load_md_content` Function\r\n\r\nThe `load_md_content` function is a helper function that reads the content of a Markdown file from the specified file path:\r\n\r\n```rust\r\nfn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\r\n    fs::read_to_string(file_path.as_ref())\r\n        .with_context(|| format!(\"Failed to read markdown file: {:?}\", file_path.as_ref()))\r\n}\nThis function takes a generic parameter P that implements the AsRef<Path> trait, allowing it to accept various types that can be converted to a file path. It uses the fs::read_to_string function to read the contents of the file and returns the content as a String. If the file cannot be read, an error is returned with additional context information.","6-implement-the-process_message-function#6. Implement the process_message Function":"The process_message function is responsible for processing user messages and generating responses using the agent:\npub async fn process_message(&self, message: &str) -> Result<String> {\r\n    self.rag_agent.prompt(message).await.map_err(anyhow::Error::from)\r\n}\nThis function takes a user message as input and passes it to the RAG agent's prompt method. The RAG agent retrieves relevant information from the knowledge base based on the user's query and generates a contextually appropriate response. The generated response is then returned as a String. If an error occurs during the processing, it is mapped to an anyhow::Error for consistent error handling.","customizing-the-knowledge-base#Customizing the Knowledge Base":"While we've used Rig's own documentation for our knowledge base, you can personalize your bot by using your own documents. Here's how:\nPrepare Your Documents: Place your Markdown files in the documents directory. Ensure they have clear and descriptive filenames.\nModify the File Paths: In rig_agent.rs, update the file paths to match your document names.\n// Example\r\nlet my_doc_path = documents_dir.join(\"my_custom_doc.md\");\r\nlet my_doc_content = Self::load_md_content(&my_doc_path)?;\nUpdate the Embeddings Builder: Adjust the EmbeddingsBuilder to include your documents.\nlet embeddings = EmbeddingsBuilder::new(embedding_model.clone())\r\n    .simple_document(\"My Custom Doc\", &my_doc_content)\r\n    .build()\r\n    .await?;\nThis way, your bot will use your own content to generate responses.","integrating-with-discord-mainrs#Integrating with Discord (main.rs)":"With our Rig agent implementation complete, it's time to connect it to Discord using the Serenity library. Serenity is an async-first Rust library for the Discord API, providing a simple and efficient way to create Discord bots.","1-modify-mainrs-to-include-the-rig-agent#1. Modify main.rs to Include the Rig Agent":"At the top of main.rs, import the necessary modules and your rig_agent:\n// main.rs\r\n\r\nmod rig_agent;\r\n\r\nuse anyhow::Result;\r\nuse serenity::async_trait;\r\nuse serenity::model::application::command::Command;\r\nuse serenity::model::application::interaction::{Interaction, InteractionResponseType};\r\nuse serenity::model::gateway::Ready;\r\nuse serenity::model::channel::Message;\r\nuse serenity::prelude::*;\r\nuse serenity::model::application::command::CommandOptionType;\r\nuse std::env;\r\nuse std::sync::Arc;\r\nuse tracing::{error, info, debug};\r\nuse rig_agent::RigAgent;\r\nuse dotenv::dotenv;\nThese imports bring in the essential types and traits from the Serenity library, as well as the RigAgent struct from the rig_agent module. The dotenv crate is used to load environment variables from a .env file.","2-store-the-bots-user-id#2. Store the Bot's User ID":"Define a key for storing the bot's user ID using Serenity's TypeMapKey trait:\nstruct BotUserId;\r\n\r\nimpl TypeMapKey for BotUserId {\r\n    type Value = serenity::model::id::UserId;\r\n}\nThis key allows us to store and retrieve the bot's user ID from Serenity's TypeMap, which is a type-safe key-value store used for sharing data across event handlers.","3-define-the-handler-struct#3. Define the Handler Struct":"Create the Handler struct that holds the RigAgent:\nstruct Handler {\r\n    rig_agent: Arc<RigAgent>,\r\n}\nThe Handler struct is responsible for handling Discord events and interactions. It contains an Arc<RigAgent>, which is a thread-safe reference-counting pointer to the Rig agent. This allows the handler to share the Rig agent across multiple event handlers without transferring ownership.","4-implement-the-eventhandler-trait#4. Implement the EventHandler Trait":"Implement the EventHandler trait for the Handler struct to define how the bot should handle various Discord events:\n#[async_trait]\r\nimpl EventHandler for Handler {\r\n    async fn interaction_create(&self, ctx: Context, interaction: Interaction) {\r\n        // ... handle interactions\r\n    }\r\n\r\n    async fn message(&self, ctx: Context, msg: Message) {\r\n        // ... handle messages\r\n    }\r\n\r\n    async fn ready(&self, ctx: Context, ready: Ready) {\r\n        // ... handle readiness\r\n    }\r\n}\nThe EventHandler trait is provided by Serenity and defines a set of methods that are called when specific events occur. In this implementation, we define three event handlers:\ninteraction_create: Called when a user interacts with the bot, such as using a slash command or clicking a button.\nmessage: Called when a message is sent in a channel where the bot is present.\nready: Called when the bot successfully connects to Discord and is ready to receive events.","handling-interactions#Handling Interactions":"In the interaction_create event handler, we process slash commands received from Discord:\nasync fn interaction_create(&self, ctx: Context, interaction: Interaction) {\r\n    debug!(\"Received an interaction\");\r\n    if let Interaction::ApplicationCommand(command) = interaction {\r\n        debug!(\"Received command: {}\", command.data.name);\r\n        let content = match command.data.name.as_str() {\r\n            \"hello\" => \"Hello! I'm your helpful Rust and Rig-powered assistant. How can I assist you today?\".to_string(),\r\n            \"ask\" => {\r\n                let query = command\r\n                    .data\r\n                    .options\r\n                    .get(0)\r\n                    .and_then(|opt| opt.value.as_ref())\r\n                    .and_then(|v| v.as_str())\r\n                    .unwrap_or(\"What would you like to ask?\");\r\n                debug!(\"Query: {}\", query);\r\n                match self.rig_agent.process_message(query).await {\r\n                    Ok(response) => response,\r\n                    Err(e) => {\r\n                        error!(\"Error processing request: {:?}\", e);\r\n                        format!(\"Error processing request: {:?}\", e)\r\n                    }\r\n                }\r\n            }\r\n            _ => \"Not implemented :(\".to_string(),\r\n        };\r\n\r\n        debug!(\"Sending response: {}\", content);\r\n\r\n        if let Err(why) = command\r\n            .create_interaction_response(&ctx.http, |response| {\r\n                response\r\n                    .kind(InteractionResponseType::ChannelMessageWithSource)\r\n                    .interaction_response_data(|message| message.content(content))\r\n            })\r\n            .await\r\n        {\r\n            error!(\"Cannot respond to slash command: {}\", why);\r\n        } else {\r\n            debug!(\"Response sent successfully\");\r\n        }\r\n    }\r\n}\nLet's break down the process:\nWhen an interaction is received, we first check if it's a slash command using the Interaction::ApplicationCommand enum variant.\nIf it's a slash command, we match on the command name to determine the appropriate action.\nFor the \"hello\" command, we respond with a simple greeting message.\nFor the \"ask\" command, we extract the user's query from the command options. If no query is provided, we use a default message.\nIf the command is \"ask\", we pass the user's query to the Rig agent's process_message method to generate a response.\nIf the Rig agent successfully generates a response, we send it back to the user.\nIf an error occurs during processing, we log the error and send an error message to the user.\nFor any other command, we respond with a \"Not implemented\" message.\nFinally, we create an interaction response using command.create_interaction_response, specifying the response type as ChannelMessageWithSource and setting the response content to the generated message.\nThis implementation allows users to interact with the bot using slash commands, providing a structured way to ask questions and receive responses from the Rig agent.","handling-messages#Handling Messages":"In the message event handler, we respond when the bot is mentioned in a message:\nasync fn message(&self, ctx: Context, msg: Message) {\r\n    if msg.mentions_me(&ctx.http).await.unwrap_or(false) {\r\n        debug!(\"Bot mentioned in message: {}\", msg.content);\r\n\r\n        let bot_id = {\r\n            let data = ctx.data.read().await;\r\n            data.get::<BotUserId>().copied()\r\n        };\r\n\r\n        if let Some(bot_id) = bot_id {\r\n            let mention = format!(\"<@{}>\", bot_id);\r\n            let content = msg.content.replace(&mention, \"\").trim().to_string();\r\n\r\n            debug!(\"Processed content after removing mention: {}\", content);\r\n\r\n            match self.rig_agent.process_message(&content).await {\r\n                Ok(response) => {\r\n                    if let Err(why) = msg.channel_id.say(&ctx.http, response).await {\r\n                        error!(\"Error sending message: {:?}\", why);\r\n                    }\r\n                }\r\n                Err(e) => {\r\n                    error!(\"Error processing message: {:?}\", e);\r\n                    if let Err(why) = msg\r\n                        .channel_id\r\n                        .say(&ctx.http, format!(\"Error processing message: {:?}\", e))\r\n                        .await\r\n                    {\r\n                        error!(\"Error sending error message: {:?}\", why);\r\n                    }\r\n                }\r\n            }\r\n        } else {\r\n            error!(\"Bot user ID not found in TypeMap\");\r\n        }\r\n    }\r\n}\nHere's how the message handling works:\nWhen a message is received, we first check if the bot is mentioned in the message using the mentions_me method.\nIf the bot is mentioned, we retrieve the bot's user ID from the TypeMap using the BotUserId key.\nIf the bot's user ID is found, we remove the mention from the message content to extract the actual query.\nWe pass the processed message content to the Rig agent's process_message method to generate a response.\nIf the Rig agent successfully generates a response, we send it back to the channel where the message was received using msg.channel_id.say.\nIf an error occurs during processing, we log the error and send an error message to the channel.\nIf the bot's user ID is not found in the TypeMap, we log an error.\nThis implementation allows users to interact with the bot by mentioning it in a message, providing a more natural way to ask questions and receive responses from the Rig agent.","handling-bot-readiness#Handling Bot Readiness":"In the ready event handler, we set up slash commands and store the bot's user ID:\nasync fn ready(&self, ctx: Context, ready: Ready) {\r\n    info!(\"{} is connected!\", ready.user.name);\r\n\r\n    {\r\n        let mut data = ctx.data.write().await;\r\n        data.insert::<BotUserId>(ready.user.id);\r\n    }\r\n\r\n    let commands = Command::set_global_application_commands(&ctx.http, |commands| {\r\n        commands\r\n            .create_application_command(|command| {\r\n                command\r\n                    .name(\"hello\")\r\n                    .description(\"Say hello to the bot\")\r\n            })\r\n            .create_application_command(|command| {\r\n                command\r\n                    .name(\"ask\")\r\n                    .description(\"Ask the bot a question\")\r\n                    .create_option(|option| {\r\n                        option\r\n                            .name(\"query\")\r\n                            .description(\"Your question for the bot\")\r\n                            .kind(CommandOptionType::String)\r\n                            .required(true)\r\n                    })\r\n            })\r\n    })\r\n    .await;\r\n\r\n    println!(\"Created the following global commands: {:#?}\", commands);\r\n}\nHere's what happens in the ready event handler:\nWhen the bot successfully connects to Discord, the ready event is triggered.\nWe log a message indicating that the bot is connected, using the bot's name from the Ready struct.\nWe store the bot's user ID in the TypeMap using the BotUserId key. This allows us to access the bot's user ID in other event handlers.\nWe create global slash commands using the Command::set_global_application_commands method.\nWe define two commands: \"hello\" and \"ask\".\nThe \"hello\" command is a simple command that greets the user.\nThe \"ask\" command allows users to ask the bot a question. It has a required \"query\" option of type String where users can input their question.\nWe print the created global commands for debugging purposes.","5-implement-the-main-function#5. Implement the main Function":"In the main function, we set up the bot and start it:\n#[tokio::main]\r\nasync fn main() -> Result<()> {\r\n    dotenv().ok();\r\n\r\n    tracing_subscriber::fmt()\r\n        .with_max_level(tracing::Level::DEBUG)\r\n        .init();\r\n\r\n    let token = env::var(\"DISCORD_TOKEN\").expect(\"Expected DISCORD_TOKEN in environment\");\r\n\r\n    let rig_agent = Arc::new(RigAgent::new().await?);\r\n\r\n    let intents = GatewayIntents::GUILD_MESSAGES\r\n        | GatewayIntents::DIRECT_MESSAGES\r\n        | GatewayIntents::MESSAGE_CONTENT;\r\n\r\n    let mut client = Client::builder(&token, intents)\r\n        .event_handler(Handler {\r\n            rig_agent: Arc::clone(&rig_agent),\r\n        })\r\n        .await\r\n        .expect(\"Err creating client\");\r\n\r\n    if let Err(why) = client.start().await {\r\n        error!(\"Client error: {:?}\", why);\r\n    }\r\n\r\n    Ok(())\r\n}\nHere's a step-by-step breakdown of the main function:\nWe load environment variables from the .env file using dotenv().ok().\nWe initialize the tracing_subscriber to set up logging with a maximum log level of DEBUG.\nWe retrieve the Discord bot token from the DISCORD_TOKEN environment variable.\nWe create a new instance of the RigAgent and wrap it in an Arc for thread-safe sharing.\nWe define the gateway intents, specifying the events we want to receive from Discord.\nWe create a new Discord client using the Client::builder method, passing in the bot token and intents.\nWe set the event handler for the client to an instance of the Handler struct, passing the RigAgent wrapped in an Arc.\nWe start the client using client.start() and handle any errors that may occur.","running-and-testing-the-bot#Running and Testing the Bot":"Now that our Discord bot is complete, let's run it and test its functionality.","1-set-up-environment-variables#1. Set Up Environment Variables":"Create a .env file in the root of your project with the following content:\nDISCORD_TOKEN=your_discord_bot_token\r\nOPENAI_API_KEY=your_openai_api_key\nReplace your_discord_bot_token with your actual Discord bot token and your_openai_api_key with your OpenAI API key.\nImportant: Never commit your .env file or API keys to version control. Add .env to your .gitignore file to prevent accidental exposure.","2-run-the-bot#2. Run the Bot":"In your terminal, navigate to the project directory and run the following command:\ncargo run\nIf everything is set up correctly, you should see logs indicating that the bot is connected and the global commands have been created.","3-invite-the-bot-to-your-discord-server#3. Invite the Bot to Your Discord Server":"To invite the bot to your Discord server, follow these steps:\nGo to the Discord Developer Portal and select your bot application.\nNavigate to the \"OAuth2\" section and click on \"URL Generator\".\nUnder \"Scopes\", select bot and applications.commands.\nUnder \"Bot Permissions\", select Send Messages, Read Message History, etc.\nCopy the generated URL and paste it into your browser.\nSelect the server you want to invite the bot to and click \"Authorize\".","4-test-the-bot#4. Test the Bot":"Once the bot is running and invited to your server, you can test its functionality:\nSlash Commands:\nType /hello to receive a greeting.\nUse /ask followed by a question to interact with the bot and receive a response generated by the Rig agent.\nMentions:\nMention the bot in a message with a question, like @BotName How do I use Rig?, and the bot will process your question and respond accordingly.\nHere's an example of the bot responding to the /ask command:","error-handling-and-logging#Error Handling and Logging":"Now that we've built and tested the bot, we need to ensure we are properly handling errors and logging our bot's behavior for improvement purposes. Rust provides powerful libraries like anyhow and tracing for error handling and logging.","1-error-handling-with-anyhow#1. Error Handling with anyhow":"The anyhow crate provides a flexible and easy-to-use error handling solution. It allows us to propagate and handle errors with additional context, making it easier to diagnose and fix issues. Here's an example of using anyhow in our rig_agent.rs file:\nuse anyhow::{Context, Result};\r\n\r\n// Example in rig_agent.rs\r\nfn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\r\n    fs::read_to_string(file_path.as_ref())\r\n        .with_context(|| format!(\"Failed to read markdown file: {:?}\", file_path.as_ref()))\r\n}\nIn this example, we use the with_context method to provide additional context to the error, specifying the file path that failed to be read. This context is included in the error message, making it easier to identify the source of the error.","2-logging-with-tracing#2. Logging with tracing":"The tracing crate provides a powerful and flexible logging solution that allows us to log messages at different levels of verbosity. Here's how we can set up logging in our main.rs file:\nuse tracing::{info, error, debug};\r\nuse tracing_subscriber;\r\n\r\n// Initialize tracing in main.rs\r\ntracing_subscriber::fmt()\r\n    .with_max_level(tracing::Level::DEBUG)\r\n    .init();\nIn this example, we initialize the tracing_subscriber with a maximum log level of DEBUG. This means that all log messages with a severity level of DEBUG or higher will be captured and displayed.Throughout our bot's code, we can use the info!, error!, and debug! macros to log messages at different severity levels, providing insights into the bot's behavior.","troubleshooting-common-issues#Troubleshooting Common Issues":"If you encounter errors, here are some common issues and how to resolve them:\nAPI Key Errors: Ensure your OpenAI API key and Discord token are correctly set in your .env file. Double-check for typos or extra spaces.\nFile Not Found: If you receive a \"Failed to read markdown file\" error, check that your document paths are correct and the files exist in the documents directory.\nDependency Conflicts: Run cargo update to ensure all dependencies are up to date.\nPermission Errors: Ensure your bot has the necessary permissions in your Discord server, such as sending messages and reading message history.","testing-and-debugging-guidance#Testing and Debugging Guidance":"","testing-the-rig-agent-independently#Testing the Rig Agent Independently":"Before integrating with Discord, you can test the Rig agent independently to ensure it's working as expected. Here's how:\n// Test the RigAgent in main.rs\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<()> {\r\n    dotenv().ok();\r\n\r\n    let rig_agent = RigAgent::new().await?;\r\n    let response = rig_agent.process_message(\"What is Rig?\").await?;\r\n    println!(\"Response: {}\", response);\r\n\r\n    Ok(())\r\n}\nRun cargo run to see the output. This test confirms that the Rig agent can process messages and generate responses based on your knowledge base.","enhancing-your-bot#Enhancing Your Bot":"Congratulations! You've built a functional AI-powered Discord bot. Now, let's explore some ways to enhance it.","1-expand-the-knowledge-base#1. Expand the Knowledge Base":"To make your bot more knowledgeable and versatile, consider adding more Markdown files to the documents directory. These files can cover a wide range of topics, from FAQs to technical documentation and beyond.","2-customize-the-bots-behavior#2. Customize the Bot's Behavior":"The behavior of your bot is largely determined by the preamble defined in the rig_agent.rs file. By adjusting this preamble, you can fine-tune how the bot interacts with users, shaping its personality, tone, and overall approach to conversations. Experiment with different preambles to find the right balance for your bot's intended purpose and audience.","3-add-more-commands#3. Add More Commands":"Slash commands provide a structured and intuitive way for users to interact with your bot. Consider implementing additional commands in the main.rs file to extend your bot's functionality. For example, you could add commands for retrieving specific information, performing calculations, or triggering automated workflows.","4-integrate-other-apis#4. Integrate Other APIs":"To further enhance your bot's capabilities, consider integrating it with other APIs. For example, you could connect your bot to a weather API to provide real-time weather updates or integrate it with a news API to deliver the latest headlines. By leveraging external APIs, you can create powerful workflows and automate tasks within your Discord server.\nCheck out our guide on building agent tools and integrating APIs here. You can also find more examples in the official Rig repo.","conclusion#Conclusion":"In this guide, we've successfully built an AI-powered Discord bot using Rust and Rig. We've learned how to set up your environment, build a language model agent, integrate with Discord, and run your bot. With this foundation, you can continue to enhance and customize your bot, transforming it into a more robust system of autonomous agents to suit your needs.See you in the next guide in the Build with Rig series!","further-resources#Further Resources":"To deepen your understanding and continue building with Rig, check out these resources:\nRig Documentation\nRig GitHub Repository\nExamples Gallery Including This Walkthrough\nOpenAI API Documentation\nUnderstanding RAG Systems\nYour Feedback Matters! We're offering a unique opportunity to shape the future of Rig:\nBuild an AI-powered application using Rig.\nShare your experience and insights via this feedback form.\nGet a chance to win $100 and have your project featured in our showcase!\nYour insights will directly influence Rig's growth. ðŸ¦€âœ¨Ad Astra,\nTachi\nCo-Founder @ Playgrounds Analytics","code-files#Code Files":"For reference, here are the complete rig_agent.rs and main.rs files.","rig_agentrs#rig_agent.rs":"// rig_agent.rs\r\n\r\nuse anyhow::{Context, Result};\r\nuse rig::providers::openai;\r\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\r\nuse rig::embeddings::EmbeddingsBuilder;\r\nuse rig::rag::RagAgent;\r\nuse std::path::Path;\r\nuse std::fs;\r\nuse std::sync::Arc;\r\n\r\n\r\npub struct RigAgent {\r\n    rag_agent: Arc<RagAgent<openai::CompletionModel, rig::vector_store::InMemoryVectorIndex<openai::EmbeddingModel>, rig::vector_store::NoIndex>>,\r\n}\r\n\r\nimpl RigAgent {\r\n    pub async fn new() -> Result<Self> {\r\n        // Initialize OpenAI client\r\n        let openai_client = openai::Client::from_env();\r\n        let embedding_model = openai_client.embedding_model(\"text-embedding-3-small\");\r\n\r\n        // Create vector store\r\n        let mut vector_store = InMemoryVectorStore::default();\r\n\r\n        // Get the current directory and construct paths to markdown files\r\n        let current_dir = std::env::current_dir()?;\r\n        let documents_dir = current_dir.join(\"documents\");\r\n\r\n        let md1_path = documents_dir.join(\"Rig_guide.md\");\r\n        let md2_path = documents_dir.join(\"Rig_faq.md\");\r\n        let md3_path = documents_dir.join(\"Rig_examples.md\");\r\n\r\n        // Load markdown documents\r\n        let md1_content = Self::load_md_content(&md1_path)?;\r\n        let md2_content = Self::load_md_content(&md2_path)?;\r\n        let md3_content = Self::load_md_content(&md3_path)?;\r\n\r\n        // Create embeddings and add to vector store\r\n        let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\r\n            .simple_document(\"Rig_guide\", &md1_content)\r\n            .simple_document(\"Rig_faq\", &md2_content)\r\n            .simple_document(\"Rig_examples\", &md3_content)\r\n            .build()\r\n            .await?;\r\n\r\n        vector_store.add_documents(embeddings).await?;\r\n\r\n        // Create index\r\n        let context_index = vector_store.index(embedding_model);\r\n\r\n        // Create RAG agent\r\n        let rag_agent = Arc::new(openai_client.context_rag_agent(\"gpt-4\")\r\n            .preamble(\"You are an advanced AI assistant powered by [Rig](https://rig.rs/), a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base.\r\n\r\n                    Key responsibilities and behaviors:\r\n                    1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information.\r\n                    2. Clarity and Conciseness: Provide clear and concise answers. Ensure responses are short and to the point. Use bullet points or numbered lists for complex information when appropriate.\r\n                    3. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information.\r\n                    4. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example:\r\n                        \\`\\`\\`rust\r\n                        let example_code = \\\"This is how you format Rust code for Discord\\\";\r\n                        println!(\\\"{}\\\", example_code);\r\n                        \\`\\`\\`\r\n                    \")\r\n            .dynamic_context(2, context_index)\r\n            .build());\r\n\r\n        Ok(Self { rag_agent })\r\n    }\r\n\r\n    pub async fn process_message(&self, message: &str) -> Result<String> {\r\n        self.rag_agent.prompt(message).await.map_err(anyhow::Error::from)\r\n    }\r\n}","mainrs#main.rs":"// main.rs\r\n\r\nmod rig_agent;\r\n\r\nuse anyhow::Result;\r\nuse serenity::async_trait;\r\nuse serenity::model::application::command::Command;\r\nuse serenity::model::application::interaction::{Interaction, InteractionResponseType};\r\nuse serenity::model::gateway::Ready;\r\nuse serenity::model::channel::Message;\r\nuse serenity::prelude::*;\r\nuse serenity::model::application::command::CommandOptionType;\r\nuse std::env;\r\nuse std::sync::Arc;\r\nuse tracing::{error, info, debug};\r\nuse rig_agent::RigAgent;\r\nuse dotenv::dotenv;\r\n\r\n// Define a key for storing the bot's user ID in the TypeMap\r\nstruct BotUserId;\r\n\r\nimpl TypeMapKey for BotUserId {\r\n    type Value = serenity::model::id::UserId;\r\n}\r\n\r\nstruct Handler {\r\n    rig_agent: Arc<RigAgent>,\r\n}\r\n\r\n#[async_trait]\r\nimpl EventHandler for Handler {\r\n    async fn interaction_create(&self, ctx: Context, interaction: Interaction) {\r\n        debug!(\"Received an interaction\");\r\n        if let Interaction::ApplicationCommand(command) = interaction {\r\n            debug!(\"Received command: {}\", command.data.name);\r\n            let content = match command.data.name.as_str() {\r\n                \"hello\" => \"Hello! I'm your helpful Rust and Rig-powered assistant. How can I assist you today?\".to_string(),\r\n                \"ask\" => {\r\n                    let query = command\r\n                        .data\r\n                        .options\r\n                        .get(0)\r\n                        .and_then(|opt| opt.value.as_ref())\r\n                        .and_then(|v| v.as_str())\r\n                        .unwrap_or(\"What would you like to ask?\");\r\n                    debug!(\"Query: {}\", query);\r\n                    match self.rig_agent.process_message(query).await {\r\n                        Ok(response) => response,\r\n                        Err(e) => {\r\n                            error!(\"Error processing request: {:?}\", e);\r\n                            format!(\"Error processing request: {:?}\", e)\r\n                        }\r\n                    }\r\n                }\r\n                _ => \"Not implemented :(\".to_string(),\r\n            };\r\n\r\n            debug!(\"Sending response: {}\", content);\r\n\r\n            if let Err(why) = command\r\n                .create_interaction_response(&ctx.http, |response| {\r\n                    response\r\n                        .kind(InteractionResponseType::ChannelMessageWithSource)\r\n                        .interaction_response_data(|message| message.content(content))\r\n                })\r\n                .await\r\n            {\r\n                error!(\"Cannot respond to slash command: {}\", why);\r\n            } else {\r\n                debug!(\"Response sent successfully\");\r\n            }\r\n        }\r\n    }\r\n\r\n    async fn message(&self, ctx: Context, msg: Message) {\r\n        if msg.mentions_me(&ctx.http).await.unwrap_or(false) {\r\n            debug!(\"Bot mentioned in message: {}\", msg.content);\r\n\r\n            let bot_id = {\r\n                let data = ctx.data.read().await;\r\n                data.get::<BotUserId>().copied()\r\n            };\r\n\r\n            if let Some(bot_id) = bot_id {\r\n                let mention = format!(\"<@{}>\", bot_id);\r\n                let content = msg.content.replace(&mention, \"\").trim().to_string();\r\n\r\n                debug!(\"Processed content after removing mention: {}\", content);\r\n\r\n                match self.rig_agent.process_message(&content).await {\r\n                    Ok(response) => {\r\n                        if let Err(why) = msg.channel_id.say(&ctx.http, response).await {\r\n                            error!(\"Error sending message: {:?}\", why);\r\n                        }\r\n                    }\r\n                    Err(e) => {\r\n                        error!(\"Error processing message: {:?}\", e);\r\n                        if let Err(why) = msg\r\n                            .channel_id\r\n                            .say(&ctx.http, format!(\"Error processing message: {:?}\", e))\r\n                            .await\r\n                        {\r\n                            error!(\"Error sending error message: {:?}\", why);\r\n                        }\r\n                    }\r\n                }\r\n            } else {\r\n                error!(\"Bot user ID not found in TypeMap\");\r\n            }\r\n        }\r\n    }\r\n\r\n    async fn ready(&self, ctx: Context, ready: Ready) {\r\n        info!(\"{} is connected!\", ready.user.name);\r\n\r\n        {\r\n            let mut data = ctx.data.write().await;\r\n            data.insert::<BotUserId>(ready.user.id);\r\n        }\r\n\r\n        let commands = Command::set_global_application_commands(&ctx.http, |commands| {\r\n            commands\r\n                .create_application_command(|command| {\r\n                    command\r\n                        .name(\"hello\")\r\n                        .description(\"Say hello to the bot\")\r\n                })\r\n                .create_application_command(|command| {\r\n                    command\r\n                        .name(\"ask\")\r\n                        .description(\"Ask the bot a question\")\r\n                        .create_option(|option| {\r\n                            option\r\n                                .name(\"query\")\r\n                                .description(\"Your question for the bot\")\r\n                                .kind(CommandOptionType::String)\r\n                                .required(true)\r\n                        })\r\n                })\r\n        })\r\n        .await;\r\n\r\n        println!(\"Created the following global commands: {:#?}\", commands);\r\n    }\r\n}\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<()> {\r\n    dotenv().ok();\r\n\r\n    tracing_subscriber::fmt()\r\n        .with_max_level(tracing::Level::DEBUG)\r\n        .init();\r\n\r\n    let token = env::var(\"DISCORD_TOKEN\").expect(\"Expected DISCORD_TOKEN in environment\");\r\n\r\n    let rig_agent = Arc::new(RigAgent::new().await?);\r\n\r\n    let intents = GatewayIntents::GUILD_MESSAGES\r\n        | GatewayIntents::DIRECT_MESSAGES\r\n        | GatewayIntents::MESSAGE_CONTENT;\r\n\r\n    let mut client = Client::builder(&token, intents)\r\n        .event_handler(Handler {\r\n            rig_agent: Arc::clone(&rig_agent),\r\n        })\r\n        .await\r\n        .expect(\"Err creating client\");\r\n\r\n    if let Err(why) = client.start().await {\r\n        error!(\"Client error: {:?}\", why);\r\n    }\r\n\r\n    Ok(())\r\n}\nThank you for following along! If you have any questions or run into issues, feel free to reach out or consult the resources provided. Happy coding!"}},"/guides/travel_agent":{"title":"Build a Flight Search AI Agent with Rig","data":{"":"TL;DR: This step-by-step guide will teach you how to build a Flight Search AI Assistant in Rust using the Rig library. By the end, you'll have a functional AI agent that can find the cheapest flights between two airports. Along the way, you'll grasp Rust fundamentals, understand how to set up AI agents with custom tools, and see how Rig simplifies the process.","introduction#Introduction":"Ever chatted with AI assistants like Siri, Alexa, or even those nifty chatbots that help you book flights or check the weather? Ever wondered what's happening under the hood? Today, we're going to demystify that by building our very own Flight Search AI Assistant using Rust and the Rig library.You might be thinking, \"Wait, Rust? Isn't that the language with the reputation for being hard?\" Don't worry! We'll walk through everything step by step, explaining concepts as we go. By the end, not only will you have a cool AI assistant, but you'll also have dipped your toes into Rust programming.Here's our game plan:\nWhy Rust and Rig? Understanding our tools of choice.\nSetting Up the Environment: Getting Rust and Rig ready to roll.\nUnderstanding Agents and Tools: The brains and hands of our assistant.\nBuilding the Flight Search Tool: Where the magic happens.\nCreating the AI Agent: Bringing our assistant to life.\nRunning and Testing: Seeing our creation in action.\nWrapping Up: Recap and next steps.\nFull source code for this project can be found on our Replit Page and Github\nSound exciting? Let's dive in!","why-rust-and-rig#Why Rust and Rig?":"","why-rust#Why Rust?":"Rust is a systems programming language known for its performance and safety. But beyond that, Rust has been making waves in areas like web development, game development, and now, AI applications. Here's why we're using Rust:\nPerformance: Rust is blazingly fast, making it ideal for applications that need to handle data quickly.\nSafety: With its strict compiler checks, Rust ensures memory safety, preventing common bugs.\nConcurrency: Rust makes it easier to write concurrent programs, which is great for handling multiple tasks simultaneously. Learn more about Rust's concurrency model.","why-rig#Why Rig?":"Rig is an open-source Rust library that simplifies building applications powered by Large Language Models (LLMs) like GPT-4. Think of Rig as a toolkit that provides:\nUnified API: It abstracts away the complexities of different LLM providers.\nHigh-Level Abstractions: Helps you build agents and tools without reinventing the wheel.\nExtensibility: You can create custom tools tailored to your application's needs.\nBy combining Rust and Rig, we're setting ourselves up to build a robust, efficient, and intelligent assistant.","setting-up-the-environment#Setting Up the Environment":"Before we start coding, let's get everything ready.","prerequisites#Prerequisites":"Install Rust: If you haven't already, install Rust by following the instructions here.\nBasic Rust Knowledge: Don't worry if you're new. We'll explain the Rust concepts as we encounter them.\nAPI Keys:\nOpenAI API Key: Sign up and get your key here.\nRapidAPI Key: We'll use this to access the trip advisor flight search API. Get it here.","project-setup#Project Setup":"","1-create-a-new-rust-project#1. Create a New Rust Project":"Open your terminal and run:\ncargo new flight_search_assistant\r\ncd flight_search_assistant\nThis initializes a new Rust project named flight_search_assistant.","2-update-cargotoml#2. Update Cargo.toml":"Open the Cargo.toml file and update it with the necessary dependencies:\n[package]\r\nname = \"flight_search_assistant\"\r\nversion = \"0.1.0\"\r\nedition = \"2021\"\r\n\r\n[dependencies]\r\nrig-core = \"0.1.0\"\r\ntokio = { version = \"1.34.0\", features = [\"full\"] }\r\nserde = { version = \"1.0\", features = [\"derive\"] }\r\nserde_json = \"1.0\"\r\nreqwest = { version = \"0.11\", features = [\"json\", \"tls\"] }\r\ndotenv = \"0.15\"\r\nthiserror = \"1.0\"\r\nchrono = { version = \"0.4\", features = [\"serde\"] }\nHere's a quick rundown:\nrig-core: The core Rig library.\ntokio: Asynchronous runtime for Rust. Think of it as the engine that allows us to perform tasks concurrently.\nserde & serde_json: Libraries for serializing and deserializing data (converting between Rust structs and JSON).\nreqwest: An HTTP client for making API requests.\ndotenv: Loads environment variables from a .env file.\nthiserror: A library for better error handling.\nchrono: For handling dates and times.","3-set-up-environment-variables#3. Set Up Environment Variables":"We don't want to hard-code our API keys for security reasons. Instead, we'll store them in a .env file.Create the file:\ntouch .env\nAdd your API keys to .env:\nOPENAI_API_KEY=your_openai_api_key_here\r\nRAPIDAPI_KEY=your_rapidapi_key_here\nRemember to replace the placeholders with your actual keys.","4-install-dependencies#4. Install Dependencies":"Back in your terminal, run:\ncargo build\nThis will download and compile all the dependencies.","understanding-agents-and-tools#Understanding Agents and Tools":"Before we jump into coding, let's clarify some key concepts.","what-are-agents#What Are Agents?":"In the context of Rig (and AI applications in general), an Agent is like the brain of your assistant. It's responsible for interpreting user inputs, deciding what actions to take, and generating responses.Think of the agent as the conductor of an orchestra, coordinating different instruments (or tools) to create harmonious music (or responses).","what-are-tools#What Are Tools?":"Tools are the skills or actions that the agent can use to fulfill a task. Each tool performs a specific function. In our case, the flight search functionality is a tool that the agent can use to find flight information.Continuing our analogy, tools are the instruments in the orchestra. Each one plays a specific role.","how-do-they-work-together#How Do They Work Together?":"When a user asks, \"Find me flights from NYC to LA,\" the agent processes this request and decides it needs to use the flight search tool to fetch the information.","building-the-flight-search-tool#Building the Flight Search Tool":"Now, let's build the tool that will handle flight searches.","1-create-the-tool-file#1. Create the Tool File":"In your src directory, create a new file named flight_search_tool.rs:\ntouch src/flight_search_tool.rs","2-import-necessary-libraries#2. Import Necessary Libraries":"Open flight_search_tool.rs and add:\nuse chrono::{DateTime, Duration, Utc};\r\nuse rig::completion::ToolDefinition;\r\nuse rig::tool::Tool;\r\nuse serde::{Deserialize, Serialize};\r\nuse serde_json::{json, Value};\r\nuse std::collections::HashMap;\r\nuse std::env;","3-define-data-structures#3. Define Data Structures":"We'll define structures to handle input arguments and output results.\n#[derive(Deserialize)]\r\npub struct FlightSearchArgs {\r\n    source: String,\r\n    destination: String,\r\n    date: Option<String>,\r\n    sort: Option<String>,\r\n    service: Option<String>,\r\n    itinerary_type: Option<String>,\r\n    adults: Option<u8>,\r\n    seniors: Option<u8>,\r\n    currency: Option<String>,\r\n    nearby: Option<String>,\r\n    nonstop: Option<String>,\r\n}\r\n\r\n#[derive(Serialize)]\r\npub struct FlightOption {\r\n    pub airline: String,\r\n    pub flight_number: String,\r\n    pub departure: String,\r\n    pub arrival: String,\r\n    pub duration: String,\r\n    pub stops: usize,\r\n    pub price: f64,\r\n    pub currency: String,\r\n    pub booking_url: String,\r\n}\nFlightSearchArgs: Represents the parameters the user provides.\nFlightOption: Represents each flight option we'll display to the user.\nWant to dive deeper? Check out Rust's struct documentation.","4-error-handling-with-thiserror#4. Error Handling with thiserror":"Rust encourages us to handle errors explicitly. We'll define a custom error type:\n#[derive(Debug, thiserror::Error)]\r\npub enum FlightSearchError {\r\n    #[error(\"HTTP request failed: {0}\")]\r\n    HttpRequestFailed(String),\r\n    #[error(\"Invalid response structure\")]\r\n    InvalidResponse,\r\n    #[error(\"API error: {0}\")]\r\n    ApiError(String),\r\n    #[error(\"Missing API key\")]\r\n    MissingApiKey,\r\n}\nThis makes it easier to manage different kinds of errors that might occur during the API call.Learn more about error handling in Rust.","5-implement-the-tool-trait#5. Implement the Tool Trait":"Now, we'll implement the Tool trait for our FlightSearchTool.First, define the tool:\npub struct FlightSearchTool;\nImplement the trait:\nimpl Tool for FlightSearchTool {\r\n    const NAME: &'static str = \"search_flights\";\r\n\r\n    type Args = FlightSearchArgs;\r\n    type Output = String;\r\n    type Error = FlightSearchError;\r\n\r\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\r\n        ToolDefinition {\r\n            name: Self::NAME.to_string(),\r\n            description: \"Search for flights between two airports\".to_string(),\r\n            parameters: json!({\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"source\": { \"type\": \"string\", \"description\": \"Source airport code (e.g., 'JFK')\" },\r\n                    \"destination\": { \"type\": \"string\", \"description\": \"Destination airport code (e.g., 'LAX')\" },\r\n                    \"date\": { \"type\": \"string\", \"description\": \"Flight date in 'YYYY-MM-DD' format\" },\r\n                },\r\n                \"required\": [\"source\", \"destination\"]\r\n            }),\r\n        }\r\n    }\r\n\r\n    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {\r\n        // We'll implement the logic for calling the flight search API next.\r\n        Ok(\"Flight search results\".to_string())\r\n    }\r\n}\ndefinition: Provides metadata about the tool.\ncall: The function that will be executed when the agent uses this tool.\nCurious about traits? Explore Rust's trait system.","6-implement-the-call-function#6. Implement the call Function":"Now, let's flesh out the call function.","a-fetch-the-api-key#a. Fetch the API Key":"let api_key = env::var(\"RAPIDAPI_KEY\").map_err(|_| FlightSearchError::MissingApiKey)?;\nWe retrieve the API key from the environment variables.","b-set-default-values#b. Set Default Values":"let date = args.date.unwrap_or_else(|| {\r\n    let date = Utc::now() + Duration::days(30);\r\n    date.format(\"%Y-%m-%d\").to_string()\r\n});\nIf the user doesn't provide a date, we'll default to 30 days from now.","c-build-query-parameters#c. Build Query Parameters":"let mut query_params = HashMap::new();\r\nquery_params.insert(\"sourceAirportCode\", args.source);\r\nquery_params.insert(\"destinationAirportCode\", args.destination);\r\nquery_params.insert(\"date\", date);","d-make-the-api-request#d. Make the API Request":"let client = reqwest::Client::new();\r\nlet response = client\r\n    .get(\"https://tripadvisor16.p.rapidapi.com/api/v1/flights/searchFlights\")\r\n    .headers({\r\n        let mut headers = reqwest::header::HeaderMap::new();\r\n        headers.insert(\"X-RapidAPI-Host\", \"tripadvisor16.p.rapidapi.com\".parse().unwrap());\r\n        headers.insert(\"X-RapidAPI-Key\", api_key.parse().unwrap());\r\n        headers\r\n    })\r\n    .query(&query_params)\r\n    .send()\r\n    .await\r\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?;\nWe use reqwest to send an HTTP GET request to the flight search API.","e-parse-and-format-the-response#e. Parse and Format the Response":"After receiving the response, we need to parse the JSON data and format it for the user.\nlet text = response\r\n    .text()\r\n    .await\r\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?;\r\n\r\nlet data: Value = serde_json::from_str(&text)\r\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?;\r\n\r\nlet mut flight_options = Vec::new();\r\n\r\n// Here, we need to extract the flight options. (It's quite detailed, so we've omitted the full code to keep the focus clear.)\r\n\r\n// Format the flight options into a readable string\r\nlet mut output = String::new();\r\noutput.push_str(\"Here are some flight options:\\n\\n\");\r\n\r\nfor (i, option) in flight_options.iter().enumerate() {\r\n    output.push_str(&format!(\"{}. **Airline**: {}\\n\", i + 1, option.airline));\r\n    // Additional formatting...\r\n}\r\n\r\nOk(output)\nNote: A lot of this section involves parsing the raw API response. To keep things concise, the detailed extraction of flight options is intentionally omitted, but in your code, you'll parse the JSON to extract the necessary fields. See the full code in the repository.Interested in JSON parsing? Check out serde_json documentation.","creating-the-ai-agent#Creating the AI Agent":"Now that our tool is ready, let's build the agent that will use it.","updating-mainrs#Updating main.rs":"Open src/main.rs and update it:\nmod flight_search_tool;\r\n\r\nuse crate::flight_search_tool::FlightSearchTool;\r\nuse dotenv::dotenv;\r\nuse rig::completion::Prompt;\r\nuse rig::providers::openai;\r\nuse std::error::Error;\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<(), Box<dyn Error>> {\r\n    dotenv().ok();\r\n\r\n    let openai_client = openai::Client::from_env();\r\n\r\n    let agent = openai_client\r\n        .agent(\"gpt-4\")\r\n        .preamble(\"You are a helpful assistant that can find flights for users.\")\r\n        .tool(FlightSearchTool)\r\n        .build();\r\n\r\n    let response = agent\r\n        .prompt(\"Find me flights from San Antonio (SAT) to Atlanta (ATL) on November 15th 2024.\")\r\n        .await?;\r\n\r\n    println!(\"Agent response:\\n{}\", response);\r\n\r\n    Ok(())\r\n}\nWe initialize the OpenAI client using our API key.\nWe create an agent, giving it a preamble (context) and adding our FlightSearchTool.\nWe prompt the agent with a query.\nFinally, we print out the response.\nWant to understand asynchronous functions? Learn about the async keyword and the #[tokio::main] macro here.","running-and-testing#Running and Testing":"Let's see our assistant in action!","build-the-project#Build the Project":"In your terminal, run:\ncargo build\nFix any compilation errors that may arise.","run-the-application#Run the Application":"cargo run\nYou should see an output similar to:\nAgent response:\r\nHere are some flight options:\r\n\r\n1. **Airline**: Spirit\r\n   - **Flight Number**: NK123\r\n   - **Departure**: 2024-11-15T05:00:00-06:00\r\n   - **Arrival**: 2024-11-15T10:12:00-05:00\r\n   - **Duration**: 4 hours 12 minutes\r\n   - **Stops**: 1 stop(s)\r\n   - **Price**: 77.97 USD\r\n   - **Booking URL**: https://www.tripadvisor.com/CheapFlightsPartnerHandoff...\r\n\r\n2. **Airline**: American\r\n   - **Flight Number**: AA456\r\n   - **Departure**: 2024-11-15T18:40:00-06:00\r\n   - **Arrival**: 2024-11-15T23:58:00-05:00\r\n   - **Duration**: 4 hours 18 minutes\r\n   - **Stops**: 1 stop(s)\r\n   - **Price**: 119.97 USD\r\n   - **Booking URL**: https://www.tripadvisor.com/CheapFlightsPartnerHandoff...\r\n\r\n...\nNote: The actual results may vary depending on the API response.","wrapping-up#Wrapping Up":"Congratulations! You've built a functional Flight Search AI Assistant using Rust and Rig. Here's what we've achieved:\nLearned Rust Basics: We've explored Rust's syntax and structure, including handling errors and asynchronous programming.\nUnderstood Agents and Tools: We learned how agents act as the brain and tools as the skills.\nBuilt a Custom Tool: We created a flight search tool that interacts with an external API.\nCreated an AI Agent: We integrated our tool into an agent that can understand and respond to user queries.\nRan and Tested Our Assistant: We saw our assistant in action, fetching and displaying flight options.","next-steps#Next Steps":"Enhance the Tool: Add more parameters like class of service, number of passengers, or price filtering.\nImprove Error Handling: Handle cases where no flights are found or when the API rate limit is reached.\nUser Interface: Build a simple command-line interface or even a web frontend."}}}